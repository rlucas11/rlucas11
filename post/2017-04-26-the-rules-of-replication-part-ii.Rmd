---
title: "The Rules of Replication: Part II"
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: /img/ceceCouchEdit.jpg
author: "Richard E. Lucas"
date: 2017-04-23
categories: 
  - Replications
tags: 
  - Replications
  - Manipulation Checks
bibliography: "~/Dropbox/MyLibraryZ2.bib"
link-citations: yes
---

In my previous [post](`r htmltools::HTML("{{< relref \"new-rules.html\" >}}")`) I focused on the question of whether replicators need to work with original authors when conducting their replication studies. I argued that this rule is based on the problematic idea that original authors somehow *own* an effect and that their reputations will be harmed if that effect turns out to be fragile or to have been a false positive. In this post, I focus on a second rule, one for which my objections may seem more controversial, at least to those who already agree that replications are valuable. 

# Rule #2: Replication Studies Should Only Be Published if Manipulation Checks Are Successful

At first glance, this seems like a good idea. The rule is designed to ensure that the replication studies that make it into the literature are done well. There is always a chance that the researchers who choose to take on a specific replication project will not be qualified to conduct the study, or that something else will go wrong to prevent the study from providing a good test of the original finding. Even guidelines for registered replication reports (guidelines that are written by advocates of replication) often stipulate that the final report will only be published if various quality checks like this are passed. And although I agree that only strong replication attempts should warrant publication, there are problems with this rule. 

`r htmltools::HTML("{{< image classes=\"fancybox fig-50 center clear\" src=\"/img/ceceCouch.jpg\"  title=\"Cece doesn't understand the rules of the couch.\" >}}")`

To provide a little context, my colleagues and I recently published a [paper](https://osf.io/preprints/psyarxiv/5fj9c) that reported nine different attempts to replicate a classic finding by @Schwarz1983JoPaSP. This original study showed that life satisfaction judgments can be strongly[^1] influenced by transient and inconsequential factors like one's mood at the time of the judgment.[^2] This original study manipulated mood in one study by asking participants to write about positive or negative life events and in a second study by contacting them on days with pleasant or unpleasant weather. Follow-up studies by these authors used additional mood manipulations including arranging it so that participants would find a dime on a copy machine [@Schwarz1987] or by contacting people after their favorite soccer team won a game [@Schwarz1987a]. The original report of this phenomenon is one of the most cited papers in social psychology and was even nominated as a "modern classic" [@schwarz2003mood], yet there are few if any replications conducted by researchers other than the original authors. Because this finding has implications for my own and my colleagues' research, we set out to replicate it with larger samples to obtain more precise estimates of the size of the effect. 

[^1]: Standardized mean differences in the range of 1 to 2.
[^2]: Though the goal of the study was to test ideas about when people are likely to rely on their mood when making other judgments, and the original studies included more conditions than our replications did so the authors could examine this important theoretical question. 

