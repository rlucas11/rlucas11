---
title: "The Rules of Replication: Part II"
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: /img/ceceCouchEdit.jpg
metaAlignment: center
author: "Richard E. Lucas"
date: 2017-04-23
categories: 
  - Replications
tags: 
  - Replications
  - Manipulation Checks
bibliography: "~/Dropbox/MyLibraryZ2.bib"
link-citations: yes
---


In my previous [post](`r htmltools::HTML("{{< relref \"new-rules.html\" >}}")`) I focused on the question of whether replicators need to work with original authors when conducting their replication studies. I argued that this rule is based on the problematic idea that original authors somehow *own* an effect and that their reputations will be harmed if that effect turns out to be fragile or to have been a false positive. In this post, I focus on a second rule, one for which my objections may seem more controversial, at least to those who already agree that replications are valuable. 

# Rule #2: Replication Studies Should Only Be Published if Manipulation Checks Are Successful

At first glance, this seems like a good idea. The rule is designed to ensure that the replication studies that make it into the literature are done well. There is always a chance that the researchers who choose to take on a specific replication project will not be qualified to conduct the study, or that something else will go wrong to prevent the study from providing a good test of the original finding. Even guidelines for registered replication reports (guidelines that are written by advocates of replication) often stipulate that the final report will only be published if various quality checks like this are passed. And although I agree that only strong replication attempts should warrant publication, there are problems with this rule. 

<br>
`r htmltools::HTML("{{< image classes=\"fancybox fig-50 center clear\" src=\"/img/ceceCouch.jpg\"  title=\"Cece doesn't understand the rules of the couch.\" >}}")`

To provide a little context, my colleagues[^*] and I recently published a [paper](https://osf.io/preprints/psyarxiv/5fj9c) that reported nine different attempts to replicate a classic finding by @Schwarz1983JoPaSP. This original study showed that life satisfaction judgments can be strongly[^1] influenced by transient and inconsequential factors like one's mood at the time of the judgment.[^2] This original study manipulated mood in one study by asking participants to write about positive or negative life events and in a second study by contacting them on days with pleasant or unpleasant weather. Follow-up studies by these authors used additional mood manipulations including arranging it so that participants would find a dime on a copy machine [@Schwarz1987], by contacting people after their favorite soccer team won a game [@Schwarz1987a], or by having participants complete a task in a pleasant or unpleasant room [@Schwarz1987a]. The original report of this phenomenon is one of the most cited papers in social psychology and was even nominated as a "modern classic" [@schwarz2003mood], yet there are few if any replications conducted by researchers other than the original authors. Because this finding has implications for my own and my colleagues' research, we set out to replicate it with larger samples to obtain more precise estimates of the size of the effect. 

[^*]: The opinions in this post are mine and may not reflect those of my co-authors.
[^1]: Standardized mean differences in the range of 1 to 2.
[^2]: Though the goal of the study was to test ideas about when people are likely to rely on their mood when making other judgments, and the original studies included more conditions than our replications did so the authors could examine this important theoretical question. 

Long story short, our results were quite different than what was found in the original study. Here's a forest plot of the original studies compared to our replication attempts:

![Meta-Analysis](/img/LSMeta.png)

And if you want to download our data to check these results yourself, feel free to visit our [OSF Page](https://osf.io/38bjg/), or just run the following code in R to instantly download all the data yourself:

```
library(httr)
library(foreign) # Only required for Study 4

## Get Data from OSF Page: https://osf.io/38bjg/

## Study 1
data <- GET('osf.io/qxbs2//?action=download',
            write_disk('study1.csv', overwrite=TRUE))
s1 <- read.csv('study1.csv')

## Study 2
data <- GET('osf.io/gdpqm//?action=download',
            write_disk('study2.csv', overwrite=TRUE))
s2 <- read.csv('study2.csv')

## Study 3
data <- GET('osf.io/drpkc//?action=download',
            write_disk('study3.csv', overwrite=TRUE))
s3 <- read.csv('study3.csv')

## Study 4
data <- GET('osf.io/dh6gk//?action=download',
            write_disk('study4.sav', overwrite=TRUE))
s4 <- read.spss('study4.sav')

## Study 5
data <- GET('osf.io/n78r3//?action=download',
            write_disk('study5.csv', overwrite=TRUE))
s5 <- read.csv('study5.csv')

## Study 6
data <- GET('osf.io/9bpuq//?action=download',
            write_disk('study6.csv', overwrite=TRUE))
s6 <- read.csv('study6.csv')

## Study 7
data <- GET('osf.io/pzqj2//?action=download',
            write_disk('study7.csv', overwrite=TRUE))
s7 <- read.csv('study7.csv')

## Study 8
data <- GET('osf.io/z5jgp//?action=download',
            write_disk('study8.csv', overwrite=TRUE))
s8 <- read.csv('study8.csv')

## Study 9
data <- GET('osf.io/nr4ps//?action=download',
            write_disk('study9.csv', overwrite=TRUE))
s9 <- read.csv('study9.csv')

## Meta-Analysis
data <- GET('osf.io/6fs3v//?action=download',
            write_disk('meta.csv', overwrite=TRUE))
meta <- read.csv('meta.csv')

```

If you take the time to read our paper (or to rerun our analyses [yourself](https://osf.io/ksprw/)), you will see that of the nine studies we conducted, the manipulation checks were only significant in four: Studies 1, 3, 5, and 6 (results for Study 2 were mixed). So would our paper be better if the published version only included these four studies? There are three reasons why I think it would not. 

#### Sampling Error Affects Manipulation Checks, Too

We often worry about publication bias, especially as it affects our understanding of effect sizes. When statistical significance is used as a filter for publication, then published effects will overestimate the true effect. Although there are important reasons to think carefully about manipulation checks when interpreting primary analyses, a blanket rule that exclude all studies from the published record when manipulation checks fail also serves as a significance filter that can distort effect size estimates. 

Consider the results from Studies 1, 2, and 3, which are pretty much exact replications of one another. As can be seen in the figures below, the manipulation checks showed weaker results in Study 2 than in Studies 1 or 3, and the manipulation check for negative mood in this study was not significant. 

```{R Mood_manip, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4}

library(httr)
library(foreign) # Only required for Study 4
library(dplyr)
library(ggplot2)
library(gridExtra)

## Get Data from OSF Page: https://osf.io/38bjg/

## Study 1
data <- GET('osf.io/qxbs2//?action=download',
            write_disk('study1.csv', overwrite=TRUE))
s1 <- read.csv('study1.csv')

## Study 2
data <- GET('osf.io/gdpqm//?action=download',
            write_disk('study2.csv', overwrite=TRUE))
s2 <- read.csv('study2.csv')

## Study 3
data <- GET('osf.io/drpkc//?action=download',
            write_disk('study3.csv', overwrite=TRUE))
s3 <- read.csv('study3.csv')

## ## Study 4
## data <- GET('osf.io/dh6gk//?action=download',
##             write_disk('study4.sav', overwrite=TRUE))
## s4 <- read.spss('study4.sav')

## ## Study 5
## data <- GET('osf.io/n78r3//?action=download',
##             write_disk('study5.csv', overwrite=TRUE))
## s5 <- read.csv('study5.csv')

## ## Study 6
## data <- GET('osf.io/9bpuq//?action=download',
##             write_disk('study6.csv', overwrite=TRUE))
## s6 <- read.csv('study6.csv')

## Study 7
data <- GET('osf.io/pzqj2//?action=download',
           write_disk('study7.csv', overwrite=TRUE))
s7 <- read.csv('study7.csv')

## ## Study 8
data <- GET('osf.io/z5jgp//?action=download',
           write_disk('study8.csv', overwrite=TRUE))
s8 <- read.csv('study8.csv')

## ## Study 9
## data <- GET('osf.io/nr4ps//?action=download',
##             write_disk('study9.csv', overwrite=TRUE))
## s9 <- read.csv('study9.csv')

## ## Meta-Analysis
## data <- GET('osf.io/6fs3v//?action=download',
##             write_disk('meta.csv', overwrite=TRUE))
## meta <- read.csv('meta.csv')


## Calculate Means, SD, and N

results <- s1 %>%
    group_by(condition) %>%
    summarise(pa_mean=mean(pa, na.rm=TRUE),
              pa_sd=sd(pa, na.rm=TRUE),
              pa_n=sum(!is.na(pa)),
              na_mean=mean(na, na.rm=TRUE),
              na_sd=sd(na, na.rm=TRUE),
              na_n=sum(!is.na(na))
              )
results$study <- "Study 1"

results2 <- s2 %>%
    group_by(condition) %>%
    summarise(pa_mean=mean(pa, na.rm=TRUE),
              pa_sd=sd(pa, na.rm=TRUE),
              pa_n=sum(!is.na(pa)),
              na_mean=mean(na, na.rm=TRUE),
              na_sd=sd(na, na.rm=TRUE),
              na_n=sum(!is.na(na))
              )
results2$study <- "Study 2"

results3 <- s3 %>%
    group_by(condition) %>%
    summarise(pa_mean=mean(pa, na.rm=TRUE),
              pa_sd=sd(pa, na.rm=TRUE),
              pa_n=sum(!is.na(pa)),
              na_mean=mean(na, na.rm=TRUE),
              na_sd=sd(na, na.rm=TRUE),
              na_n=sum(!is.na(na))
              )
results3$study <- "Study 3"

## Combine Results
results <- rbind(results,results2,results3)

## Calculate CIs
results <- results %>%
    mutate(pa_ci = (pa_sd / sqrt(pa_n)) * 2,
           na_ci = (na_sd / sqrt(na_n)) * 2
           )

## Positive Mood Plot
cilimits <- aes(ymax = pa_mean + pa_ci, ymin = pa_mean - pa_ci)
paPlot <- ggplot(results, aes(x = study, y = pa_mean, fill = condition)) +
    geom_bar(position=position_dodge(), stat="identity") +
    scale_y_continuous(limits=c(0,5),breaks=c(1:5)) +
    scale_fill_grey() +
    geom_errorbar(cilimits, width = 0.2, position=position_dodge(.9)) +
    ylab("Positive Mood") + theme_bw()

## Negative Mood Plot
cilimits <- aes(ymax = na_mean + na_ci, ymin = na_mean - na_ci)
naPlot <- ggplot(results, aes(x = study, y = na_mean, fill = condition)) +
    geom_bar(position=position_dodge(), stat="identity") +
    scale_y_continuous(limits=c(0,5),breaks=c(1:5)) +
    scale_fill_grey() +
    geom_errorbar(cilimits, width = 0.2, position=position_dodge(.9)) +
    ylab("Negative Mood") + theme_bw()

## Plot Combined Figure
grid.arrange(paPlot, naPlot, nrow=1)

```


But should we conclude that we did something wrong in Study 2, or is this the type of variability in effects that we should expect when running three exact replications of the same study? If one purpose of manipulation checks is to show that the researchers who conduct the studies are competent and the specific procedures are used, don't Studies 1 and 3 accomplish this goal? Perhaps something did go terribly wrong in Study 2, but without direct evidence that such problems occurred, it seems better to report these results and to consider the fact that the manipulation check failed when interpreting the broader results. As long as the results are available, one can examine the effect of including or excluding these studies in meta-analytic averages (as we did in our paper). 

#### Additional Researcher Degrees of Freedom

A second reason why this requirement can be problematic is that until preregistration becomes the norm for both original and replication studies, the use of failed manipulation checks as a reason for excluding studies from the published record can serve as an additional "researcher degree of freedom" that can increase rates of false positive findings in the literature. For instance, in one paper in the original series of studies on this effect, @Strack1985JoPaSP conducted three studies with designs similar to ours. Two of the three manipulation checks failed, yet all three studies---studies that supported the idea that the experimental manipulation successfully influenced life satisfaction judgments---were reported and interpreted. I'm not saying that this decision was wrong, but if rules about using manipulation checks to dismiss study results are applied inconsistently, then this additional researcher degree of freedom can (even unintentionally) lead to an overly positive picture of the evidence for an effect. So the danger is that manipulation checks will only be interpreted as an indicator of study quality when the study fails to find the expected effect. Again, transparency with appropriately nuanced conclusions seems to be a better approach. 

#### Sometimes the Replication is About the Manipulation

Ours is not the first paper to raise questions about the robustness of results reported in the original @Schwarz1983JoPaSP paper. In a prior [study](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3695376/), we used data from almost a million people to test the extent to which weather conditions were associated with reports of life satisfaction [@lucas2011weather]. Despite the extremely high power of this study we found no meaningful effects. @simonsohn_small_2015 discussed these results in his paper on how replications should be interpreted, and @schwarz_evaluating_2016 challenged Simonsohn's argument that our study could even be considered a replication of @Schwarz1983JoPaSP. The gist of their argument was that they, in the original study, were not interested in the effects of *weather* on life satisfaction but in the effects of *mood* on life satisfaction;  weather was simply used as a convenient way to manipulate mood. Therefore, studies that do not assess mood (or that use mood induction procedures that do not successfully change people's mood) tell us little about the robustness of the original finding. 

The problem is that there are strong reasons to be skeptical that weather does affect mood, especially so strongly that it could result in effects as large as those found in the original study. I think that many of us have the intuition that our mood improves when the weather is nice. Indeed, Schwarz and Clore's intuitions were so strong that they thought it unnecessary to pretest the effectiveness of this manipulation or to identify prior literature that supported their intuition. In their look back on these studies they noted: 

>  In retrospect, we are surprised by how “painless” these experiments were. Unless our memories fail us, we did not conduct extensive pretests, were spared poor results and new starts, and had the good luck of “things falling into place” on the first trial [@schwarz2003mood, p. 298]

But as detailed in our paper [@lucas2011weather] and in a great blog post by [Tal Yarkoni](http://www.talyarkoni.org/blog/2015/05/18/the-mysterious-inefficacy-of-weather/), it's actually really difficult to find empirical evidence that weather is linked with mood. Most studies that have looked haven't found much there.  So an important goal of our nine-study paper was to test the replicability of the mood effect on life satisfaction *and* to test the replicability of manipulations like weather on mood. Both pieces of evidence help evaluate past research in this area. 

So did we find evidence that weather affects mood? Across two studies with much larger samples than the original, the answer was consistently "no": 


```{r weather, echo=FALSE, warning=FALSE, message=FALSE, fig.height=4}
#### Weather Studies

## Calculate Means, SD, and N

results <- s7 %>%
    group_by(condition) %>%
    summarise(pa_mean=mean(moodPA, na.rm=TRUE),
              pa_sd=sd(moodPA, na.rm=TRUE),
              pa_n=sum(!is.na(moodPA)),
              na_mean=mean(moodNA, na.rm=TRUE),
              na_sd=sd(moodNA, na.rm=TRUE),
              na_n=sum(!is.na(moodNA))
              )
results$study <- "Study 7"

results2 <- s8 %>%
    filter(surveyCond == "NoQuestions") %>%
    group_by(condition) %>%
    summarise(pa_mean=mean(moodPA, na.rm=TRUE),
              pa_sd=sd(moodPA, na.rm=TRUE),
              pa_n=sum(!is.na(moodPA)),
              na_mean=mean(moodNA, na.rm=TRUE),
              na_sd=sd(moodNA, na.rm=TRUE),
              na_n=sum(!is.na(moodNA))
              )
results2$study <- "Study 8"


## Combine Results
results <- rbind(results,results2)
results <- results %>%
    filter(!is.na(condition))

## Calculate CIs
results <- results %>%
    mutate(pa_ci = (pa_sd / sqrt(pa_n)) * 2,
           na_ci = (na_sd / sqrt(na_n)) * 2
           )

## Positive Mood Plot
cilimits <- aes(ymax = pa_mean + pa_ci, ymin = pa_mean - pa_ci)
paPlot <- ggplot(results, aes(x = study, y = pa_mean, fill = condition)) +
    geom_bar(position=position_dodge(), stat="identity") +
    scale_y_continuous(limits=c(0,5),breaks=c(1:5)) +
    scale_fill_grey() +
    geom_errorbar(cilimits, width = 0.2, position=position_dodge(.9)) +
    ylab("Positive Mood") + theme_bw()


## Negative Mood Plot
cilimits <- aes(ymax = na_mean + na_ci, ymin = na_mean - na_ci)
naPlot <- ggplot(results, aes(x = study, y = na_mean, fill = condition)) +
    geom_bar(position=position_dodge(), stat="identity") +
    scale_y_continuous(limits=c(0,5),breaks=c(1:5)) +
    scale_fill_grey() +
    geom_errorbar(cilimits, width = 0.2, position=position_dodge(.9)) +
    ylab("Negative Mood") + theme_bw()


grid.arrange(paPlot, naPlot, nrow=1)

```

It is absolutely true that because these mood inductions failed, these two studies cannot be used to answer the question of whether mood affects life satisfaction judgments. They can, however, be used to update our beliefs about the strength of existing evidence for such effects. An important brick in the wall of evidence used to support the idea that mood affects life satisfaction judgments comes from a single study that used weather as a manipulation. Until additional replications can additional replications can be conducted, our results suggest that that particular brick may be weak. 

#### Summary

It goes without saying that replication studies can only be interpreted if they are done well. Manipulation checks represent one way that the quality of the study and expertise of the investigators can be tested. However, a blanket rule that replications can be ignored, and perhaps even prevented from being published, if manipulation checks fail is problematic. In most cases, transparent reporting of all results is probably a better default. 

#### References
